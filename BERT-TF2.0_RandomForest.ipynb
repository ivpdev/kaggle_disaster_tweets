{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2IjSWx7-O8yY"
   },
   "source": [
    "(based on https://towardsdatascience.com/simple-bert-using-tensorflow-2-0-132cb19e9b22)\n",
    "\n",
    "# Kaggle Disaster Tweets Challenge\n",
    "## BERT Embeddings with TensorFlow 2.0 + Random Forest\n",
    "\n",
    "https://www.kaggle.com/c/nlp-getting-started\n",
    "\n",
    "With the new release of TensorFlow, this Notebook aims to show a simple use of the BERT model.\n",
    "- See BERT on paper: https://arxiv.org/pdf/1810.04805.pdf\n",
    "- See BERT on GitHub: https://github.com/google-research/bert\n",
    "- See BERT on TensorHub: https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\n",
    "- See 'old' use of BERT for comparison: https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SQktrOSAPq_n"
   },
   "source": [
    "## Update TF\n",
    "We need Tensorflow 2.0 and TensorHub 0.7 for this Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Iwew0KP8vRM",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ivp/dev/work/ai/bert/kaggle_disaster/venv_bert4/bin/pip\r\n"
     ]
    }
   ],
   "source": [
    "#!pip install tensorflow==2.0\n",
    "#!pip install tensorflow_hub==0.7\n",
    "#!pip install bert-for-tf2\n",
    "#!pip install sentencepiece\n",
    "#!pip install pandas\n",
    "#!pip install category_encoders==2.1.0\n",
    "!which pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "NV-yr9ulP_E-",
    "outputId": "c37c1b21-1aa5-456e-ddc4-446636805189"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version:  2.0.0\n",
      "Hub version:  0.7.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "print(\"TF version: \", tf.__version__)\n",
    "print(\"Hub version: \", hub.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4svE1x7iP3af"
   },
   "source": [
    "If TensorFlow Hub is not 0.7 yet on release, use dev:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pUo1k6rd9iaP"
   },
   "outputs": [],
   "source": [
    "### !pip install tf-hub-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "av1oBm8m-Ajz"
   },
   "outputs": [],
   "source": [
    "# hub.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cMeXU54uQUew"
   },
   "source": [
    "## Prepare BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KBfktvAc-CNb"
   },
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import bert\n",
    "FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "from tensorflow.keras.models import Model       # Keras is the new high level API for TensorFlow\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tU2OpvYrRFNf"
   },
   "source": [
    "Building model using tf.keras and hub. from sentences to embeddings.\n",
    "\n",
    "Inputs:\n",
    " - input token ids (tokenizer converts tokens using vocab file)\n",
    " - input masks (1 for useful tokens, 0 for padding)\n",
    " - segment ids (for 2 text training: 0 for the first one, 1 for the second one)\n",
    "\n",
    "Outputs:\n",
    " - pooled_output of shape `[batch_size, 768]` with representations for the entire input sequences\n",
    " - sequence_output of shape `[batch_size, max_seq_length, 768]` with representations for each input token (in context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IW6V3afD-q1K"
   },
   "outputs": [],
   "source": [
    "max_seq_length = 128  # Your choice here.\n",
    "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                       name=\"input_word_ids\")\n",
    "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                   name=\"input_mask\")\n",
    "segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                    name=\"segment_ids\")\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=True)\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gmR3jHYE_y3X"
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=[pooled_output, sequence_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('modeldump.h5')\n",
    "#tf.keras.experimental.export_saved_model(model, 'modeldump2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    " #model2 = tf.keras.models.load_model('modeldump2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_model = tf.keras.experimental.load_from_saved_model('modeldump2.h5', custom_objects={'KerasLayer':hub.KerasLayer})\n",
    "#print(reloaded_model.get_config())\n",
    "\n",
    "#Get input shape from model.get_config()\n",
    "#reloaded_model.build((None, 224, 224, 3))\n",
    "#reloaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reloaded_model.build((None, 224, 224, 3))\n",
    "#reloaded_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BFDpzy1-STOh"
   },
   "source": [
    "Generating segments and masks based on the original BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e4Y_r3lmFO1E"
   },
   "outputs": [],
   "source": [
    "# See BERT paper: https://arxiv.org/pdf/1810.04805.pdf\n",
    "# And BERT implementation convert_single_example() at https://github.com/google-research/bert/blob/master/run_classifier.py\n",
    "\n",
    "def get_masks(tokens, max_seq_length):\n",
    "    \"\"\"Mask for padding\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def get_segments(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    segments = []\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def get_ids(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44u2pruZSbMX"
   },
   "source": [
    "Import tokenizer using the original vocab file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sm3lGfQb-1J8"
   },
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./data/input/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idfy():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def freq(lst):\n",
    "    d = {}\n",
    "    for i in lst:\n",
    "        if d.get(i):\n",
    "            d[i] += 1\n",
    "        else:\n",
    "            d[i] = 1\n",
    "    return d\n",
    "\n",
    "def series_to_list(series):\n",
    "    return list(chain.from_iterable(series.values))\n",
    "\n",
    "#len(set(tags_lowercased))\n",
    "## sorted(list(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tags_list = series_to_list(train[\"hashtags\"])\n",
    "#tags_lowercased = list(map(lambda tag: tag.lower(), tags_list))\n",
    "\n",
    "#freq(tags_lowercased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = train[\"location\"].apply(lambda x: isinstance(x, float))\n",
    "\n",
    "len(train[~m])\n",
    "locations  = list(train[~m][\"location\"])\n",
    "\n",
    "\n",
    "freq(locations)\n",
    "set(locations)\n",
    "\n",
    "\n",
    "#ddd = add_location_columns(train[~m])\n",
    "\n",
    "ddd1 = add_location_columns(train[m])\n",
    "#train[~m]\n",
    "#train[m][\"location\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tokens(text):\n",
    "    #TODO tags to separate column\n",
    "    #TODO strip hash from tags in text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "    \n",
    "    input_ids = get_ids(tokens, tokenizer, max_seq_length)\n",
    "    input_masks = get_masks(tokens, max_seq_length)\n",
    "    input_segments = get_segments(tokens, max_seq_length)\n",
    "    \n",
    "    #print('tokens')\n",
    "    #print(tokens)\n",
    "    #print('input_ids')\n",
    "    #print(input_ids)\n",
    "    #print('input_masks')\n",
    "    #print(input_masks)\n",
    "    #print('input_segments')\n",
    "    #print(input_segments)\n",
    "    \n",
    "    pool_embs, all_embs = model.predict([[input_ids],[input_masks],[input_segments]])\n",
    "    \n",
    "    #print('pool_embs')\n",
    "    #print(pool_embs)\n",
    "    \n",
    "    return pool_embs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_hashtags(text):\n",
    "    r = \"#\\S*\"\n",
    "    return re.findall(r, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime \n",
    "\n",
    "def vectorize_text_in_daraframe(df, text_column, result_column):\n",
    "    start = time.time()\n",
    "    print(\"started text vectorization\")\n",
    "    datetime.fromtimestamp(start).strftime(\"%A, %B %d, %Y %I:%M:%S\")\n",
    "    \n",
    "    df[result_column] = df[text_column].map(vectorize)\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"ended text vectorization\")\n",
    "    datetime.fromtimestamp(end).strftime(\"%A, %B %d, %Y %I:%M:%S\")\n",
    "\n",
    "    print(\"time elapsed\")\n",
    "    print(str(round(end - start)) + \" seconds\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode categorical column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "#TODO reviews.region_2.fillna(\"Unknown\")\n",
    "\n",
    "def fill_na(df, colname):\n",
    "    return df[[colname]].fillna(\"Unknown\")[colname]\n",
    "\n",
    "\n",
    "def encode_categorical_column(df, colname, encoder=None):\n",
    "    name_no_nan = colname + '_no_nan'\n",
    "    df[name_no_nan] = fill_na(df, colname) #df[colname].map(lambda x: x if type(x)==\"str\" else \"NA\")\n",
    "    if encoder:\n",
    "        hash_columns = encoder.transform(df[name_no_nan])\n",
    "    else:    \n",
    "        encoder = ce.HashingEncoder(cols = [name_no_nan], verbose=1)\n",
    "        hash_columns = encoder.fit_transform(df[name_no_nan], df['target'])\n",
    "    \n",
    "    hash_columns.index = df.index\n",
    "    #print(hash_columns)\n",
    "    hash_col_names = list(map(lambda col_name: colname + '_' + col_name, list(hash_columns.columns.values)))\n",
    "    hash_columns.columns = hash_col_names\n",
    "    #print(hash_columns)\n",
    "    #return df.join(hash_columns)\n",
    "    return pd.concat([df,hash_columns], axis=1), encoder\n",
    "\n",
    "#def encode_categorical_columns(df, colnames):\n",
    "#    res = df\n",
    "#    for colname in colnames:\n",
    "#        res = encode_categorical_column(res, colname)\n",
    "#    \n",
    "#    return res\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"hashtags\"] = train[\"text\"].map(extract_hashtags)\n",
    "\n",
    "#train[\"hashtags_vector\"] = tf_idfy(train[\"hashtags\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small = train.sample(2000)\n",
    "#train_small = train.sample(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def append_col_to_np_array(df, col, arr):\n",
    "    col_arr = df[col].to_numpy()\n",
    "    col_arr1 = col_arr.reshape(col_arr.shape[0],1)\n",
    "    \n",
    "    res = numpy.append(arr, col_arr1, axis=1)\n",
    "    return res\n",
    "    \n",
    "def append_cols_to_np_array(df, cols, arr):\n",
    "    res = arr\n",
    "    for col in cols:\n",
    "        res = append_col_to_np_array(df, col, res)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_column(df_from, df_to, colname):\n",
    "    df_to[colname] = df_from[colname]\n",
    "    return df_to\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_feature_array(df):\n",
    "    x = numpy.stack(df[\"vectorized\"], axis=0)\n",
    "    \n",
    "    x = append_cols_to_np_array(df, ['location_col_0', 'location_col_1', 'location_col_2',\n",
    "       'location_col_3', 'location_col_4', 'location_col_5', 'location_col_6',\n",
    "       'location_col_7'], x)\n",
    "    \n",
    "    x = append_cols_to_np_array(df, ['keyword_col_0', 'keyword_col_1',\n",
    "       'keyword_col_2', 'keyword_col_3', 'keyword_col_4', 'keyword_col_5',\n",
    "       'keyword_col_6', 'keyword_col_7'], x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "class FeaturPreparationResult:\n",
    "  def __init__(self, df, encoder_location, encoder_keyword):\n",
    "     self.df = df\n",
    "     self.encoder_location = encoder_location\n",
    "     self.encoder_keyword = encoder_keyword\n",
    "\n",
    "def prepare_features(df, encoder_location=None, encoder_keyword=None):\n",
    "    print(\"preparing location columns\")\n",
    "    df1, encoder_location = encode_categorical_column(df, 'location', encoder_location)\n",
    "    df2, encoder_keyword = encode_categorical_column(df1, 'keyword', encoder_keyword)\n",
    "    \n",
    "    print(\"vectorizing text\")\n",
    "    df3 = vectorize_text_in_daraframe(df2, \"text\", \"vectorized\")\n",
    "    \n",
    "    return FeaturPreparationResult(df3, encoder_location, encoder_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing location columns\n",
      "vectorizing text\n",
      "started text vectorization\n",
      "ended text vectorization\n",
      "time elapsed\n",
      "1199 seconds\n"
     ]
    }
   ],
   "source": [
    "feat_prep_result = prepare_features(train_small)\n",
    "\n",
    "train_small_prepared = feat_prep_result.df\n",
    "encoder_location = feat_prep_result.encoder_location\n",
    "encoder_keyword = feat_prep_result.encoder_keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = to_feature_array(train_small_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small_prepared.to_csv(\"./data/work/trained-vectorized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_small_prepared[\"target\"].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "^C\n",
      "\u001b[31mOperation cancelled by user\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=1000, n_jobs=-1, random_state=42) #TODO params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "                       n_jobs=-1, oob_score=False, random_state=42, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00      1154\\n           1       1.00      1.00      1.00       846\\n\\n    accuracy                           1.00      2000\\n   macro avg       1.00      1.00      1.00      2000\\nweighted avg       1.00      1.00      1.00      2000\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "classification_report(y, clf.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"./data/input/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3263"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing location columns\n",
      "vectorizing text\n",
      "started text vectorization\n",
      "ended text vectorization\n",
      "time elapsed\n",
      "1797 seconds\n"
     ]
    }
   ],
   "source": [
    "feat_prep_result_test = prepare_features(test, encoder_location, encoder_keyword)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prepared = feat_prep_result_test.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = to_feature_array(test_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prepared.to_csv(\"./data/work/test-vectorized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3263"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_to_predict = test[\"vectorized\"]\n",
    "#X_to_predict = np.vstack(X_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_to_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3263"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"target\"] = predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "(test[[\"id\", \"target\"]]).to_csv(\"./data/output/result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  target\n",
       "0         0       0\n",
       "1         2       0\n",
       "2         3       0\n",
       "3         9       1\n",
       "4        11       1\n",
       "...     ...     ...\n",
       "3258  10861       0\n",
       "3259  10865       1\n",
       "3260  10868       1\n",
       "3261  10874       1\n",
       "3262  10875       1\n",
       "\n",
       "[3263 rows x 2 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "BERT Embeddings with TensorFlow 2.0",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py36bert",
   "language": "python",
   "name": "py36bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
