{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2IjSWx7-O8yY"
   },
   "source": [
    "(based on https://towardsdatascience.com/simple-bert-using-tensorflow-2-0-132cb19e9b22)\n",
    "\n",
    "# Kaggle Disaster Tweets Challenge\n",
    "## BERT Embeddings with TensorFlow 2.0 + Random Forest\n",
    "\n",
    "https://www.kaggle.com/c/nlp-getting-started\n",
    "\n",
    "With the new release of TensorFlow, this Notebook aims to show a simple use of the BERT model.\n",
    "- See BERT on paper: https://arxiv.org/pdf/1810.04805.pdf\n",
    "- See BERT on GitHub: https://github.com/google-research/bert\n",
    "- See BERT on TensorHub: https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\n",
    "- See 'old' use of BERT for comparison: https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SQktrOSAPq_n"
   },
   "source": [
    "## Update TF\n",
    "We need Tensorflow 2.0 and TensorHub 0.7 for this Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Iwew0KP8vRM",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.0 in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (2.0.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from tensorflow==2.0) (1.11.2)\n",
      "Requirement already satisfied: gast==0.2.2 in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from tensorflow==2.0) (0.2.2)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from tensorflow==2.0) (0.9.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from tensorflow==2.0) (1.0.8)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from tensorflow==2.0) (0.1.8)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from tensorflow==2.0) (3.1.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from tensorflow==2.0) (3.11.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from tensorflow==2.0) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from tensorflow==2.0) (2.0.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from tensorflow==2.0) (0.33.6)\n",
      "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from tensorflow==2.0) (2.0.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from tensorflow==2.0) (1.13.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from tensorflow==2.0) (1.18.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from tensorflow==2.0) (1.26.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from tensorflow==2.0) (1.1.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from tensorflow==2.0) (0.8.1)\n",
      "Requirement already satisfied: h5py in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from keras-applications>=1.0.8->tensorflow==2.0) (2.10.0)\n",
      "Requirement already satisfied: setuptools in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow==2.0) (42.0.2.post20191203)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.4.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.1.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.16.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (1.10.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (2.22.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (1.3.0)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.2.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (1.25.7)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.4.8)\n",
      "Requirement already satisfied: tensorflow_hub==0.7 in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (0.7.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from tensorflow_hub==0.7) (1.13.0)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from tensorflow_hub==0.7) (3.11.2)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from tensorflow_hub==0.7) (1.18.0)\n",
      "Requirement already satisfied: setuptools in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from protobuf>=3.4.0->tensorflow_hub==0.7) (42.0.2.post20191203)\n",
      "Requirement already satisfied: bert-for-tf2 in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (0.12.6)\n",
      "Requirement already satisfied: py-params>=0.7.3 in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from bert-for-tf2) (0.8.2)\n",
      "Requirement already satisfied: params-flow>=0.7.1 in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from bert-for-tf2) (0.7.4)\n",
      "Requirement already satisfied: numpy in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from params-flow>=0.7.1->bert-for-tf2) (1.18.0)\n",
      "Requirement already satisfied: tqdm in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from params-flow>=0.7.1->bert-for-tf2) (4.41.0)\n",
      "Requirement already satisfied: sentencepiece in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (0.1.85)\n",
      "Requirement already satisfied: pandas in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (0.25.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from pandas) (1.18.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas) (1.13.0)\n"
     ]
    }
   ],
   "source": [
    "#!pip install tensorflow==2.0\n",
    "#!pip install tensorflow_hub==0.7\n",
    "#!pip install bert-for-tf2\n",
    "#!pip install sentencepiece\n",
    "#!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "NV-yr9ulP_E-",
    "outputId": "c37c1b21-1aa5-456e-ddc4-446636805189"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version:  2.0.0\n",
      "Hub version:  0.7.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "print(\"TF version: \", tf.__version__)\n",
    "print(\"Hub version: \", hub.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4svE1x7iP3af"
   },
   "source": [
    "If TensorFlow Hub is not 0.7 yet on release, use dev:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pUo1k6rd9iaP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf-hub-nightly in ./venv_bert3/lib/python3.7/site-packages (0.8.0.dev201912250004)\n",
      "Requirement already satisfied: numpy>=1.12.0 in ./venv_bert3/lib/python3.7/site-packages (from tf-hub-nightly) (1.18.0)\n",
      "Requirement already satisfied: six>=1.12.0 in ./venv_bert3/lib/python3.7/site-packages (from tf-hub-nightly) (1.13.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in ./venv_bert3/lib/python3.7/site-packages (from tf-hub-nightly) (3.11.2)\n",
      "Requirement already satisfied: setuptools in ./venv_bert3/lib/python3.7/site-packages (from protobuf>=3.8.0->tf-hub-nightly) (42.0.2)\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "### !pip install tf-hub-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "av1oBm8m-Ajz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.8.0.dev'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hub.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cMeXU54uQUew"
   },
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KBfktvAc-CNb"
   },
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import bert\n",
    "FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "from tensorflow.keras.models import Model       # Keras is the new high level API for TensorFlow\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tU2OpvYrRFNf"
   },
   "source": [
    "Building model using tf.keras and hub. from sentences to embeddings.\n",
    "\n",
    "Inputs:\n",
    " - input token ids (tokenizer converts tokens using vocab file)\n",
    " - input masks (1 for useful tokens, 0 for padding)\n",
    " - segment ids (for 2 text training: 0 for the first one, 1 for the second one)\n",
    "\n",
    "Outputs:\n",
    " - pooled_output of shape `[batch_size, 768]` with representations for the entire input sequences\n",
    " - sequence_output of shape `[batch_size, max_seq_length, 768]` with representations for each input token (in context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IW6V3afD-q1K"
   },
   "outputs": [],
   "source": [
    "max_seq_length = 128  # Your choice here.\n",
    "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                       name=\"input_word_ids\")\n",
    "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                   name=\"input_mask\")\n",
    "segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                    name=\"segment_ids\")\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=True)\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gmR3jHYE_y3X"
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=[pooled_output, sequence_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BFDpzy1-STOh"
   },
   "source": [
    "Generating segments and masks based on the original BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e4Y_r3lmFO1E"
   },
   "outputs": [],
   "source": [
    "# See BERT paper: https://arxiv.org/pdf/1810.04805.pdf\n",
    "# And BERT implementation convert_single_example() at https://github.com/google-research/bert/blob/master/run_classifier.py\n",
    "\n",
    "def get_masks(tokens, max_seq_length):\n",
    "    \"\"\"Mask for padding\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def get_segments(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    segments = []\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def get_ids(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44u2pruZSbMX"
   },
   "source": [
    "Import tokenizer using the original vocab file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sm3lGfQb-1J8"
   },
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"./data/input/train.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7613"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tokens(text):\n",
    "    #TODO tags to separate column\n",
    "    #TODO strip hash from tags in text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "    \n",
    "    input_ids = get_ids(tokens, tokenizer, max_seq_length)\n",
    "    input_masks = get_masks(tokens, max_seq_length)\n",
    "    input_segments = get_segments(tokens, max_seq_length)\n",
    "    \n",
    "    #print('tokens')\n",
    "    #print(tokens)\n",
    "    #print('input_ids')\n",
    "    #print(input_ids)\n",
    "    #print('input_masks')\n",
    "    #print(input_masks)\n",
    "    #print('input_segments')\n",
    "    #print(input_segments)\n",
    "    \n",
    "    pool_embs, all_embs = model.predict([[input_ids],[input_masks],[input_segments]])\n",
    "    \n",
    "    #print('pool_embs')\n",
    "    #print(pool_embs)\n",
    "    \n",
    "    return pool_embs[0]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train[\"tokenized\"] = train[\"text\"].map(tokenize)\n",
    "train_small = train.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small[\"vectorized\"] = train_small[\"text\"].map(vectorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>vectorized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1866</th>\n",
       "      <td>2682</td>\n",
       "      <td>crush</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Seriously have the biggest girl crush ever on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.8701796, -0.2937034, 0.064057834, 0.674443...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3923</th>\n",
       "      <td>5579</td>\n",
       "      <td>flood</td>\n",
       "      <td>New York</td>\n",
       "      <td>12' 72W CREE LED Work Light Bar Alloy Spot Flo...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.7649183, -0.6727237, -0.97098035, 0.736852...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>1106</td>\n",
       "      <td>blew%20up</td>\n",
       "      <td>Florida</td>\n",
       "      <td>@iphooey @TIME Ironically Michele Bachmann bro...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.714161, -0.29003528, -0.53335494, 0.405214...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6408</th>\n",
       "      <td>9161</td>\n",
       "      <td>suicide%20bomber</td>\n",
       "      <td>19.600858, -99.047821</td>\n",
       "      <td>Mosque bombing strikes Saudi special forces; a...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.7877648, -0.5819566, -0.9204984, 0.6665516...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5082</th>\n",
       "      <td>7248</td>\n",
       "      <td>nuclear%20disaster</td>\n",
       "      <td>Austin TX</td>\n",
       "      <td>Alarming Rise in Dead Marine Life Since the #F...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.9089203, -0.743991, -0.99118143, 0.8479957...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7040</th>\n",
       "      <td>10087</td>\n",
       "      <td>typhoon</td>\n",
       "      <td>Wilmington, Delaware</td>\n",
       "      <td>Map: Typhoon Soudelor's predicted path as it a...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.7030923, -0.50600356, -0.9014137, 0.522852...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2584</th>\n",
       "      <td>3708</td>\n",
       "      <td>destroyed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@alanhahn @HDumpty39 Daughtery would get destr...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.6311881, -0.42598, -0.86701566, 0.41932264...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3480</th>\n",
       "      <td>4975</td>\n",
       "      <td>explosion</td>\n",
       "      <td>S.F. Bay area</td>\n",
       "      <td>MORE--&amp;gt;OSHA officers on siteinvestigating N...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.8061329, -0.5781096, -0.9546388, 0.6799465...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1625</th>\n",
       "      <td>2348</td>\n",
       "      <td>collapse</td>\n",
       "      <td>Pompano Beach, FL</td>\n",
       "      <td>Growth dries up for BHP Billiton as oil price ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.8894482, -0.6992881, -0.9840752, 0.8223380...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2137</th>\n",
       "      <td>3068</td>\n",
       "      <td>deaths</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "      <td>Hear @DrFriedenCDC talk on how to avoid thousa...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.7241185, -0.5487765, -0.9166135, 0.5508809...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2760</th>\n",
       "      <td>3964</td>\n",
       "      <td>devastation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#HungerArticles: Nepal: Rebuilding Lives and L...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.85695076, -0.6548193, -0.9375647, 0.782579...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>960</td>\n",
       "      <td>blaze</td>\n",
       "      <td>Durham N.C</td>\n",
       "      <td>@GuiltyGearXXACP yeah I know but blaze blue do...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.82067615, -0.5799226, -0.9788534, 0.773645...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5487</th>\n",
       "      <td>7830</td>\n",
       "      <td>quarantine</td>\n",
       "      <td>NYC, New York</td>\n",
       "      <td>Reddit Will Now Quarantine Offensive Content: ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.7186744, -0.5015608, -0.91443235, 0.503726...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>104</td>\n",
       "      <td>accident</td>\n",
       "      <td>Walker County, Alabama</td>\n",
       "      <td>Reported motor vehicle accident in Curry on He...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.83221847, -0.5344279, -0.9331895, 0.686464...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2426</th>\n",
       "      <td>3486</td>\n",
       "      <td>derailed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relief train carrying survivors of the deraile...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.8525612, -0.51749223, -0.8383615, 0.681728...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2099</th>\n",
       "      <td>3015</td>\n",
       "      <td>death</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I feel like death</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.80855566, -0.17480159, 0.6150435, 0.543005...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6721</th>\n",
       "      <td>9629</td>\n",
       "      <td>thunderstorm</td>\n",
       "      <td>Oklahoma City, OK</td>\n",
       "      <td>Severe Thunderstorm Warnings have been cancell...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.9408881, -0.57161176, -0.9856629, 0.891455...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5797</th>\n",
       "      <td>8274</td>\n",
       "      <td>rioting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT : Why Sweden Isn't Venezuela: There have be...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.51198864, -0.33901975, -0.7895248, 0.27757...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4749</th>\n",
       "      <td>6757</td>\n",
       "      <td>lightning</td>\n",
       "      <td>Norman, Oklahoma</td>\n",
       "      <td>Couple storms near Guthrie OK. Leaving Norman ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.91358757, -0.5951615, -0.9723917, 0.800662...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6688</th>\n",
       "      <td>9582</td>\n",
       "      <td>thunder</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thunder lightening torrential rain and a power...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.90318066, -0.4153637, -0.9085038, 0.743044...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>860</td>\n",
       "      <td>bioterror</td>\n",
       "      <td>Pelham, AL</td>\n",
       "      <td>Thank you @FedEx for no longer shipping live m...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.8998849, -0.60312176, -0.9712116, 0.804386...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4989</th>\n",
       "      <td>7118</td>\n",
       "      <td>military</td>\n",
       "      <td>somewhere USA</td>\n",
       "      <td>i strongly support our military &amp;amp; their fa...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.68981117, -0.619199, -0.9723066, 0.6251591...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3333</th>\n",
       "      <td>4774</td>\n",
       "      <td>evacuated</td>\n",
       "      <td>Denver, Colorado</td>\n",
       "      <td>13000 evacuated as California firefighters fig...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.7894868, -0.57940465, -0.9784067, 0.717060...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2280</th>\n",
       "      <td>3274</td>\n",
       "      <td>demolish</td>\n",
       "      <td>Fruit Bowl</td>\n",
       "      <td>@XGN_Infinity @Ronin_Carbon HAHAH Mutual host ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.6363959, -0.2008667, 0.30063316, 0.3682762...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>2854</td>\n",
       "      <td>damage</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Drop it down on a nigga do damage ! ??</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.8024616, -0.4536845, -0.78612006, 0.730169...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4225</th>\n",
       "      <td>6000</td>\n",
       "      <td>hazardous</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#dam #gms Olap #world pres: http://t.co/mIcjNP...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.76333606, -0.61441374, -0.9860074, 0.76829...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6241</th>\n",
       "      <td>8913</td>\n",
       "      <td>snowstorm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@dirk_trossen \\n\\nI've still got some of the s...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.85808283, -0.44885793, -0.73772556, 0.5187...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3521</th>\n",
       "      <td>5034</td>\n",
       "      <td>eyewitness</td>\n",
       "      <td>NaN</td>\n",
       "      <td>How ÛÏLittle BoyÛ Affected the People In Hi...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.86013377, -0.60285836, -0.9584067, 0.76626...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4779</th>\n",
       "      <td>6800</td>\n",
       "      <td>loud%20bang</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>tkyonly1fmk: Breaking news! Unconfirmed! I jus...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.717222, -0.4467946, -0.9495872, 0.44239616...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7323</th>\n",
       "      <td>10483</td>\n",
       "      <td>wild%20fires</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My heart goes out to all those effected by the...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.87653947, -0.44839397, -0.8949327, 0.65597...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6507</th>\n",
       "      <td>9303</td>\n",
       "      <td>survive</td>\n",
       "      <td>Fountain City, IN</td>\n",
       "      <td>Ended today's staff meeting with the teacher v...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.84718007, -0.45459768, -0.8441591, 0.62160...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6311</th>\n",
       "      <td>9022</td>\n",
       "      <td>stretcher</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@invalid @Grazed @Towel @Stretcher @PLlolz @wi...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.8970225, -0.4176898, -0.7990818, 0.7948659...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1402</th>\n",
       "      <td>2023</td>\n",
       "      <td>casualties</td>\n",
       "      <td>NaN</td>\n",
       "      <td>disinfo: Warfighting Robots Could Reduce Civil...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.6991051, -0.4550188, -0.8722464, 0.5133350...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7412</th>\n",
       "      <td>10605</td>\n",
       "      <td>wounded</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Have you ever seen the President \\nwho killed ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.6692669, -0.2968574, -0.7967632, 0.4980999...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3070</th>\n",
       "      <td>4405</td>\n",
       "      <td>electrocute</td>\n",
       "      <td>NaN</td>\n",
       "      <td>when you got an extension cord that extends fr...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.7485085, -0.32328865, -0.54522425, 0.39239...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4905</th>\n",
       "      <td>6981</td>\n",
       "      <td>massacre</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sousse beach massacre linked to Tunis museum a...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.81703967, -0.62510675, -0.955207, 0.732922...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>852</td>\n",
       "      <td>bioterror</td>\n",
       "      <td>Oxford, MS</td>\n",
       "      <td>Hmm...this could be problem for some researche...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.70851666, -0.33048227, -0.8205363, 0.41989...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6750</th>\n",
       "      <td>9670</td>\n",
       "      <td>tornado</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Second tornado confirmed in SundayÛªs storm h...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.8198851, -0.53296316, -0.9760173, 0.769519...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4784</th>\n",
       "      <td>6807</td>\n",
       "      <td>loud%20bang</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>kotolily_: Breaking news! Unconfirmed! I just ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.707609, -0.44690368, -0.961409, 0.4803359,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>1129</td>\n",
       "      <td>blight</td>\n",
       "      <td>Calgary, Alberta</td>\n",
       "      <td>@Daorcey @nsit_ YOUR a great pair. Like a coup...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.75520283, -0.50047284, -0.9073705, 0.73837...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>5675</td>\n",
       "      <td>floods</td>\n",
       "      <td>California</td>\n",
       "      <td>Floods Fishing Finally Sunshine &amp;amp; Fab Deal...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.7745119, -0.5831489, -0.9616414, 0.7160529...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7018</th>\n",
       "      <td>10059</td>\n",
       "      <td>typhoon</td>\n",
       "      <td>Savage States of America</td>\n",
       "      <td>Map: Typhoon Soudelor's predicted path as it a...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.7053625, -0.45680684, -0.8810951, 0.521768...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1187</th>\n",
       "      <td>1709</td>\n",
       "      <td>bridge%20collapse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ashes 2015: Australia collapse at Trent Bridge...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.71952933, -0.5480302, -0.9480675, 0.588167...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7237</th>\n",
       "      <td>10365</td>\n",
       "      <td>weapons</td>\n",
       "      <td>rural ohio (fuck)</td>\n",
       "      <td>@eyecuts @Erasuterism I love 96 Gal Deco to de...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.61359686, -0.4825636, -0.9490604, 0.486708...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3549</th>\n",
       "      <td>5073</td>\n",
       "      <td>famine</td>\n",
       "      <td>Washington, D.C.</td>\n",
       "      <td>A memorial to the millions who perished in the...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.82937175, -0.61708826, -0.9478217, 0.64642...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7187</th>\n",
       "      <td>10297</td>\n",
       "      <td>weapon</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>BUT I will be uploading these videos ASAP so y...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.7614376, -0.4775986, -0.90559804, 0.482396...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4489</th>\n",
       "      <td>6385</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>NaN</td>\n",
       "      <td>You messed up my feeling like a hurricane dama...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.8274906, -0.2440308, 0.14641857, 0.6288112...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5904</th>\n",
       "      <td>8432</td>\n",
       "      <td>sandstorm</td>\n",
       "      <td>USA</td>\n",
       "      <td>Watch This Airport Get Swallowed Up By A Sands...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.8658073, -0.56960267, -0.8924712, 0.731879...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6772</th>\n",
       "      <td>9704</td>\n",
       "      <td>tornado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I feel like a tornado http://t.co/iZJK6kpWiZ</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.81723875, -0.5384982, -0.9109171, 0.683873...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7204</th>\n",
       "      <td>10320</td>\n",
       "      <td>weapon</td>\n",
       "      <td>www.twitch.tv/PKSparkxx</td>\n",
       "      <td>Slosher is GOAT. Freaking love that weapon. Ca...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.8605595, -0.45796132, -0.89698875, 0.70278...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id             keyword                  location  \\\n",
       "1866   2682               crush                       NaN   \n",
       "3923   5579               flood                  New York   \n",
       "765    1106           blew%20up                   Florida   \n",
       "6408   9161    suicide%20bomber     19.600858, -99.047821   \n",
       "5082   7248  nuclear%20disaster                 Austin TX   \n",
       "7040  10087             typhoon      Wilmington, Delaware   \n",
       "2584   3708           destroyed                       NaN   \n",
       "3480   4975           explosion             S.F. Bay area   \n",
       "1625   2348            collapse         Pompano Beach, FL   \n",
       "2137   3068              deaths               Atlanta, GA   \n",
       "2760   3964         devastation                       NaN   \n",
       "665     960               blaze               Durham N.C    \n",
       "5487   7830          quarantine             NYC, New York   \n",
       "72      104            accident    Walker County, Alabama   \n",
       "2426   3486            derailed                       NaN   \n",
       "2099   3015               death                       NaN   \n",
       "6721   9629        thunderstorm         Oklahoma City, OK   \n",
       "5797   8274             rioting                       NaN   \n",
       "4749   6757           lightning          Norman, Oklahoma   \n",
       "6688   9582             thunder                       NaN   \n",
       "595     860           bioterror                Pelham, AL   \n",
       "4989   7118            military            somewhere USA    \n",
       "3333   4774           evacuated          Denver, Colorado   \n",
       "2280   3274            demolish                Fruit Bowl   \n",
       "1983   2854              damage                       NaN   \n",
       "4225   6000           hazardous                       NaN   \n",
       "6241   8913           snowstorm                       NaN   \n",
       "3521   5034          eyewitness                       NaN   \n",
       "4779   6800         loud%20bang                     Kenya   \n",
       "7323  10483        wild%20fires                       NaN   \n",
       "6507   9303             survive        Fountain City, IN    \n",
       "6311   9022           stretcher                       NaN   \n",
       "1402   2023          casualties                       NaN   \n",
       "7412  10605             wounded                       NaN   \n",
       "3070   4405         electrocute                       NaN   \n",
       "4905   6981            massacre                       NaN   \n",
       "590     852           bioterror                Oxford, MS   \n",
       "6750   9670             tornado                    Canada   \n",
       "4784   6807         loud%20bang                     Kenya   \n",
       "780    1129              blight          Calgary, Alberta   \n",
       "3996   5675              floods                California   \n",
       "7018  10059             typhoon  Savage States of America   \n",
       "1187   1709   bridge%20collapse                       NaN   \n",
       "7237  10365             weapons         rural ohio (fuck)   \n",
       "3549   5073              famine          Washington, D.C.   \n",
       "7187  10297              weapon             Massachusetts   \n",
       "4489   6385           hurricane                       NaN   \n",
       "5904   8432           sandstorm                       USA   \n",
       "6772   9704             tornado                       NaN   \n",
       "7204  10320              weapon   www.twitch.tv/PKSparkxx   \n",
       "\n",
       "                                                   text  target  \\\n",
       "1866  Seriously have the biggest girl crush ever on ...       0   \n",
       "3923  12' 72W CREE LED Work Light Bar Alloy Spot Flo...       0   \n",
       "765   @iphooey @TIME Ironically Michele Bachmann bro...       0   \n",
       "6408  Mosque bombing strikes Saudi special forces; a...       1   \n",
       "5082  Alarming Rise in Dead Marine Life Since the #F...       1   \n",
       "7040  Map: Typhoon Soudelor's predicted path as it a...       1   \n",
       "2584  @alanhahn @HDumpty39 Daughtery would get destr...       0   \n",
       "3480  MORE--&gt;OSHA officers on siteinvestigating N...       1   \n",
       "1625  Growth dries up for BHP Billiton as oil price ...       0   \n",
       "2137  Hear @DrFriedenCDC talk on how to avoid thousa...       0   \n",
       "2760  #HungerArticles: Nepal: Rebuilding Lives and L...       1   \n",
       "665   @GuiltyGearXXACP yeah I know but blaze blue do...       0   \n",
       "5487  Reddit Will Now Quarantine Offensive Content: ...       0   \n",
       "72    Reported motor vehicle accident in Curry on He...       1   \n",
       "2426  Relief train carrying survivors of the deraile...       1   \n",
       "2099                                  I feel like death       0   \n",
       "6721  Severe Thunderstorm Warnings have been cancell...       1   \n",
       "5797  RT : Why Sweden Isn't Venezuela: There have be...       1   \n",
       "4749  Couple storms near Guthrie OK. Leaving Norman ...       1   \n",
       "6688  Thunder lightening torrential rain and a power...       1   \n",
       "595   Thank you @FedEx for no longer shipping live m...       0   \n",
       "4989  i strongly support our military &amp; their fa...       0   \n",
       "3333  13000 evacuated as California firefighters fig...       1   \n",
       "2280  @XGN_Infinity @Ronin_Carbon HAHAH Mutual host ...       0   \n",
       "1983             Drop it down on a nigga do damage ! ??       0   \n",
       "4225  #dam #gms Olap #world pres: http://t.co/mIcjNP...       1   \n",
       "6241  @dirk_trossen \\n\\nI've still got some of the s...       1   \n",
       "3521  How ÛÏLittle BoyÛ Affected the People In Hi...       1   \n",
       "4779  tkyonly1fmk: Breaking news! Unconfirmed! I jus...       0   \n",
       "7323  My heart goes out to all those effected by the...       1   \n",
       "6507  Ended today's staff meeting with the teacher v...       0   \n",
       "6311  @invalid @Grazed @Towel @Stretcher @PLlolz @wi...       0   \n",
       "1402  disinfo: Warfighting Robots Could Reduce Civil...       1   \n",
       "7412  Have you ever seen the President \\nwho killed ...       0   \n",
       "3070  when you got an extension cord that extends fr...       0   \n",
       "4905  Sousse beach massacre linked to Tunis museum a...       1   \n",
       "590   Hmm...this could be problem for some researche...       1   \n",
       "6750  Second tornado confirmed in SundayÛªs storm h...       1   \n",
       "4784  kotolily_: Breaking news! Unconfirmed! I just ...       0   \n",
       "780   @Daorcey @nsit_ YOUR a great pair. Like a coup...       0   \n",
       "3996  Floods Fishing Finally Sunshine &amp; Fab Deal...       0   \n",
       "7018  Map: Typhoon Soudelor's predicted path as it a...       1   \n",
       "1187  Ashes 2015: Australia collapse at Trent Bridge...       1   \n",
       "7237  @eyecuts @Erasuterism I love 96 Gal Deco to de...       1   \n",
       "3549  A memorial to the millions who perished in the...       1   \n",
       "7187  BUT I will be uploading these videos ASAP so y...       0   \n",
       "4489  You messed up my feeling like a hurricane dama...       0   \n",
       "5904  Watch This Airport Get Swallowed Up By A Sands...       1   \n",
       "6772       I feel like a tornado http://t.co/iZJK6kpWiZ       1   \n",
       "7204  Slosher is GOAT. Freaking love that weapon. Ca...       0   \n",
       "\n",
       "                                             vectorized  \n",
       "1866  [-0.8701796, -0.2937034, 0.064057834, 0.674443...  \n",
       "3923  [-0.7649183, -0.6727237, -0.97098035, 0.736852...  \n",
       "765   [-0.714161, -0.29003528, -0.53335494, 0.405214...  \n",
       "6408  [-0.7877648, -0.5819566, -0.9204984, 0.6665516...  \n",
       "5082  [-0.9089203, -0.743991, -0.99118143, 0.8479957...  \n",
       "7040  [-0.7030923, -0.50600356, -0.9014137, 0.522852...  \n",
       "2584  [-0.6311881, -0.42598, -0.86701566, 0.41932264...  \n",
       "3480  [-0.8061329, -0.5781096, -0.9546388, 0.6799465...  \n",
       "1625  [-0.8894482, -0.6992881, -0.9840752, 0.8223380...  \n",
       "2137  [-0.7241185, -0.5487765, -0.9166135, 0.5508809...  \n",
       "2760  [-0.85695076, -0.6548193, -0.9375647, 0.782579...  \n",
       "665   [-0.82067615, -0.5799226, -0.9788534, 0.773645...  \n",
       "5487  [-0.7186744, -0.5015608, -0.91443235, 0.503726...  \n",
       "72    [-0.83221847, -0.5344279, -0.9331895, 0.686464...  \n",
       "2426  [-0.8525612, -0.51749223, -0.8383615, 0.681728...  \n",
       "2099  [-0.80855566, -0.17480159, 0.6150435, 0.543005...  \n",
       "6721  [-0.9408881, -0.57161176, -0.9856629, 0.891455...  \n",
       "5797  [-0.51198864, -0.33901975, -0.7895248, 0.27757...  \n",
       "4749  [-0.91358757, -0.5951615, -0.9723917, 0.800662...  \n",
       "6688  [-0.90318066, -0.4153637, -0.9085038, 0.743044...  \n",
       "595   [-0.8998849, -0.60312176, -0.9712116, 0.804386...  \n",
       "4989  [-0.68981117, -0.619199, -0.9723066, 0.6251591...  \n",
       "3333  [-0.7894868, -0.57940465, -0.9784067, 0.717060...  \n",
       "2280  [-0.6363959, -0.2008667, 0.30063316, 0.3682762...  \n",
       "1983  [-0.8024616, -0.4536845, -0.78612006, 0.730169...  \n",
       "4225  [-0.76333606, -0.61441374, -0.9860074, 0.76829...  \n",
       "6241  [-0.85808283, -0.44885793, -0.73772556, 0.5187...  \n",
       "3521  [-0.86013377, -0.60285836, -0.9584067, 0.76626...  \n",
       "4779  [-0.717222, -0.4467946, -0.9495872, 0.44239616...  \n",
       "7323  [-0.87653947, -0.44839397, -0.8949327, 0.65597...  \n",
       "6507  [-0.84718007, -0.45459768, -0.8441591, 0.62160...  \n",
       "6311  [-0.8970225, -0.4176898, -0.7990818, 0.7948659...  \n",
       "1402  [-0.6991051, -0.4550188, -0.8722464, 0.5133350...  \n",
       "7412  [-0.6692669, -0.2968574, -0.7967632, 0.4980999...  \n",
       "3070  [-0.7485085, -0.32328865, -0.54522425, 0.39239...  \n",
       "4905  [-0.81703967, -0.62510675, -0.955207, 0.732922...  \n",
       "590   [-0.70851666, -0.33048227, -0.8205363, 0.41989...  \n",
       "6750  [-0.8198851, -0.53296316, -0.9760173, 0.769519...  \n",
       "4784  [-0.707609, -0.44690368, -0.961409, 0.4803359,...  \n",
       "780   [-0.75520283, -0.50047284, -0.9073705, 0.73837...  \n",
       "3996  [-0.7745119, -0.5831489, -0.9616414, 0.7160529...  \n",
       "7018  [-0.7053625, -0.45680684, -0.8810951, 0.521768...  \n",
       "1187  [-0.71952933, -0.5480302, -0.9480675, 0.588167...  \n",
       "7237  [-0.61359686, -0.4825636, -0.9490604, 0.486708...  \n",
       "3549  [-0.82937175, -0.61708826, -0.9478217, 0.64642...  \n",
       "7187  [-0.7614376, -0.4775986, -0.90559804, 0.482396...  \n",
       "4489  [-0.8274906, -0.2440308, 0.14641857, 0.6288112...  \n",
       "5904  [-0.8658073, -0.56960267, -0.8924712, 0.731879...  \n",
       "6772  [-0.81723875, -0.5384982, -0.9109171, 0.683873...  \n",
       "7204  [-0.8605595, -0.45796132, -0.89698875, 0.70278...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_small[\"vectorized\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_small[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/ivp/Library/Caches/pip/wheels/76/03/bb/589d421d27431bcd2c6da284d5f2286c8e3b2ea3cf1594c074/sklearn-0.0-py2.py3-none-any.whl\n",
      "Collecting scikit-learn\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/83/ff/d8e912e96aa47abc4ffb02bb3d05eaee45c14b74d02f0abf22b97d83a888/scikit_learn-0.22-cp36-cp36m-macosx_10_6_intel.whl (11.1MB)\n",
      "\u001b[K     |████████████████████████████████| 11.1MB 1.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in /Users/ivp/opt/anaconda3/envs/py36bert/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.18.0)\n",
      "Collecting joblib>=0.11\n",
      "  Using cached https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl\n",
      "Collecting scipy>=0.17.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/94/cd76305a69fff844e83655ed7b254835df4eddd5fc0e2d0eb2914501b36e/scipy-1.4.1-cp36-cp36m-macosx_10_6_intel.whl (28.5MB)\n",
      "\u001b[K     |████████████████████████████████| 28.5MB 971kB/s eta 0:00:01    |█▋                              | 1.5MB 1.1MB/s eta 0:00:24     |██████████▌                     | 9.3MB 1.9MB/s eta 0:00:11     |████████████████████▎           | 18.0MB 500kB/s eta 0:00:21     |███████████████████████▏        | 20.6MB 1.1MB/s eta 0:00:07\n",
      "\u001b[?25hInstalling collected packages: joblib, scipy, scikit-learn, sklearn\n",
      "Successfully installed joblib-0.14.1 scikit-learn-0.22 scipy-1.4.1 sklearn-0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=1000, n_jobs=-1, random_state=42) #TODO params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = len(X.values[0])\n",
    "num_observations = len(X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.vstack(X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1 = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "                       n_jobs=-1, oob_score=False, random_state=42, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X1, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00        24\\n           1       1.00      1.00      1.00        26\\n\\n    accuracy                           1.00        50\\n   macro avg       1.00      1.00      1.00        50\\nweighted avg       1.00      1.00      1.00        50\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "classification_report(y1, clf.predict(X1))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "BERT Embeddings with TensorFlow 2.0",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py36bert",
   "language": "python",
   "name": "py36bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
