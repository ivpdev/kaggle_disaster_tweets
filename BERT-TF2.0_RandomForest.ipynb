{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2IjSWx7-O8yY"
   },
   "source": [
    "(based on https://towardsdatascience.com/simple-bert-using-tensorflow-2-0-132cb19e9b22)\n",
    "\n",
    "# Kaggle Disaster Tweets Challenge\n",
    "## BERT Embeddings with TensorFlow 2.0 + Random Forest\n",
    "\n",
    "https://www.kaggle.com/c/nlp-getting-started\n",
    "\n",
    "With the new release of TensorFlow, this Notebook aims to show a simple use of the BERT model.\n",
    "- See BERT on paper: https://arxiv.org/pdf/1810.04805.pdf\n",
    "- See BERT on GitHub: https://github.com/google-research/bert\n",
    "- See BERT on TensorHub: https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\n",
    "- See 'old' use of BERT for comparison: https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SQktrOSAPq_n"
   },
   "source": [
    "## Update TF\n",
    "We need Tensorflow 2.0 and TensorHub 0.7 for this Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Iwew0KP8vRM",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install tensorflow==2.0\n",
    "#!pip install tensorflow_hub==0.7\n",
    "#!pip install bert-for-tf2\n",
    "#!pip install sentencepiece\n",
    "#!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "NV-yr9ulP_E-",
    "outputId": "c37c1b21-1aa5-456e-ddc4-446636805189"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version:  2.0.0\n",
      "Hub version:  0.7.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "print(\"TF version: \", tf.__version__)\n",
    "print(\"Hub version: \", hub.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4svE1x7iP3af"
   },
   "source": [
    "If TensorFlow Hub is not 0.7 yet on release, use dev:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pUo1k6rd9iaP"
   },
   "outputs": [],
   "source": [
    "### !pip install tf-hub-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "av1oBm8m-Ajz"
   },
   "outputs": [],
   "source": [
    "# hub.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cMeXU54uQUew"
   },
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KBfktvAc-CNb"
   },
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import bert\n",
    "FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "from tensorflow.keras.models import Model       # Keras is the new high level API for TensorFlow\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tU2OpvYrRFNf"
   },
   "source": [
    "Building model using tf.keras and hub. from sentences to embeddings.\n",
    "\n",
    "Inputs:\n",
    " - input token ids (tokenizer converts tokens using vocab file)\n",
    " - input masks (1 for useful tokens, 0 for padding)\n",
    " - segment ids (for 2 text training: 0 for the first one, 1 for the second one)\n",
    "\n",
    "Outputs:\n",
    " - pooled_output of shape `[batch_size, 768]` with representations for the entire input sequences\n",
    " - sequence_output of shape `[batch_size, max_seq_length, 768]` with representations for each input token (in context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IW6V3afD-q1K"
   },
   "outputs": [],
   "source": [
    "max_seq_length = 128  # Your choice here.\n",
    "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                       name=\"input_word_ids\")\n",
    "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                   name=\"input_mask\")\n",
    "segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                    name=\"segment_ids\")\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=True)\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gmR3jHYE_y3X"
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=[pooled_output, sequence_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('modeldump.h5')\n",
    "#tf.keras.experimental.export_saved_model(model, 'modeldump2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    " #model2 = tf.keras.models.load_model('modeldump2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_model = tf.keras.experimental.load_from_saved_model('modeldump2.h5', custom_objects={'KerasLayer':hub.KerasLayer})\n",
    "#print(reloaded_model.get_config())\n",
    "\n",
    "#Get input shape from model.get_config()\n",
    "#reloaded_model.build((None, 224, 224, 3))\n",
    "#reloaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 768), (None, 109482241   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 109,482,241\n",
      "Trainable params: 109,482,240\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#reloaded_model.build((None, 224, 224, 3))\n",
    "reloaded_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BFDpzy1-STOh"
   },
   "source": [
    "Generating segments and masks based on the original BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e4Y_r3lmFO1E"
   },
   "outputs": [],
   "source": [
    "# See BERT paper: https://arxiv.org/pdf/1810.04805.pdf\n",
    "# And BERT implementation convert_single_example() at https://github.com/google-research/bert/blob/master/run_classifier.py\n",
    "\n",
    "def get_masks(tokens, max_seq_length):\n",
    "    \"\"\"Mask for padding\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def get_segments(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    segments = []\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def get_ids(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44u2pruZSbMX"
   },
   "source": [
    "Import tokenizer using the original vocab file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sm3lGfQb-1J8"
   },
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"./data/input/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tokens(text):\n",
    "    #TODO tags to separate column\n",
    "    #TODO strip hash from tags in text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "    \n",
    "    input_ids = get_ids(tokens, tokenizer, max_seq_length)\n",
    "    input_masks = get_masks(tokens, max_seq_length)\n",
    "    input_segments = get_segments(tokens, max_seq_length)\n",
    "    \n",
    "    #print('tokens')\n",
    "    #print(tokens)\n",
    "    #print('input_ids')\n",
    "    #print(input_ids)\n",
    "    #print('input_masks')\n",
    "    #print(input_masks)\n",
    "    #print('input_segments')\n",
    "    #print(input_segments)\n",
    "    \n",
    "    pool_embs, all_embs = model.predict([[input_ids],[input_masks],[input_segments]])\n",
    "    \n",
    "    #print('pool_embs')\n",
    "    #print(pool_embs)\n",
    "    \n",
    "    return pool_embs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def vectorize(df, text_column, result_column):\n",
    "    start = time.time()\n",
    "print(\"started\")\n",
    "print(start)\n",
    "\n",
    "train_small[\"vectorized\"] = train_small[\"text\"].map(vectorize)\n",
    "\n",
    "end = time.time()\n",
    "print(\"ended\")\n",
    "print(end)\n",
    "\n",
    "print(\"time elapsed\")\n",
    "print(end - start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train[\"tokenized\"] = train[\"text\"].map(tokenize)\n",
    "train_small = train.sample(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started\n",
      "1578513534.959518\n",
      "ended\n",
      "1578514365.737824\n",
      "time elapsed\n",
      "830.7783060073853\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "start = time.time()\n",
    "print(\"started\")\n",
    "print(start)\n",
    "\n",
    "train_small[\"vectorized\"] = train_small[\"text\"].map(vectorize)\n",
    "\n",
    "end = time.time()\n",
    "print(\"ended\")\n",
    "print(end)\n",
    "\n",
    "print(\"time elapsed\")\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small.to_csv(\"./data/work/trained-vectorized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small2 = pd.read_csv(\"./data/work/trained-vectorized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>vectorized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3637</th>\n",
       "      <td>5186</td>\n",
       "      <td>fatalities</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-??-\\n; kitana\\n? her fatalities slay me\\nÛÓk...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.807587, -0.5216827, -0.8024632, 0.69634426...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>1638</td>\n",
       "      <td>bombing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The only country claiming the moral high groun...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.8450406, -0.6705742, -0.9464853, 0.7645291...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5088</th>\n",
       "      <td>7255</td>\n",
       "      <td>nuclear%20disaster</td>\n",
       "      <td>Marbella. Spain</td>\n",
       "      <td>http://t.co/GaM7otGISw\\nANOTHER DISASTER WAITI...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.7676656, -0.44996068, -0.8376965, 0.653483...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>882</td>\n",
       "      <td>bioterrorism</td>\n",
       "      <td>OES 4th Point. sisSTAR &amp; TI</td>\n",
       "      <td>I liked a @YouTube video http://t.co/XO2ZbPBJB...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.5953094, -0.39899623, -0.8330214, 0.410473...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6550</th>\n",
       "      <td>9373</td>\n",
       "      <td>survived</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Well me and dad survived my driving ????????</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.8154489, -0.5718806, -0.94418716, 0.745205...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5145</th>\n",
       "      <td>7337</td>\n",
       "      <td>nuclear%20reactor</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Salem 2 nuclear reactor shut down over electri...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.8021079, -0.5850645, -0.96756816, 0.676609...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5635</th>\n",
       "      <td>8036</td>\n",
       "      <td>refugees</td>\n",
       "      <td>NaN</td>\n",
       "      <td>./.....hmm 12000 Nigerian refugees repatriated...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.85007364, -0.56564945, -0.9505792, 0.77903...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3023</th>\n",
       "      <td>4341</td>\n",
       "      <td>dust%20storm</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "      <td>@deadlydemi even staying up all night to he ba...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.39099488, -0.285781, -0.84145904, 0.274905...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>1237</td>\n",
       "      <td>blood</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A friend is like blood they are not beside us ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.9279789, -0.540044, -0.9374038, 0.8930397,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5715</th>\n",
       "      <td>8156</td>\n",
       "      <td>rescuers</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>@AndyGilder Channel 5 have been doing the same...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.7831831, -0.44527027, -0.930498, 0.5956335...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id             keyword                     location  \\\n",
       "3637  5186          fatalities                          NaN   \n",
       "1137  1638             bombing                          NaN   \n",
       "5088  7255  nuclear%20disaster              Marbella. Spain   \n",
       "611    882        bioterrorism  OES 4th Point. sisSTAR & TI   \n",
       "6550  9373            survived                          NaN   \n",
       "...    ...                 ...                          ...   \n",
       "5145  7337   nuclear%20reactor                          NaN   \n",
       "5635  8036            refugees                          NaN   \n",
       "3023  4341        dust%20storm                  Atlanta, GA   \n",
       "855   1237               blood                          NaN   \n",
       "5715  8156            rescuers                   Birmingham   \n",
       "\n",
       "                                                   text  target  \\\n",
       "3637  -??-\\n; kitana\\n? her fatalities slay me\\nÛÓk...       1   \n",
       "1137  The only country claiming the moral high groun...       1   \n",
       "5088  http://t.co/GaM7otGISw\\nANOTHER DISASTER WAITI...       1   \n",
       "611   I liked a @YouTube video http://t.co/XO2ZbPBJB...       1   \n",
       "6550       Well me and dad survived my driving ????????       0   \n",
       "...                                                 ...     ...   \n",
       "5145  Salem 2 nuclear reactor shut down over electri...       1   \n",
       "5635  ./.....hmm 12000 Nigerian refugees repatriated...       1   \n",
       "3023  @deadlydemi even staying up all night to he ba...       1   \n",
       "855   A friend is like blood they are not beside us ...       0   \n",
       "5715  @AndyGilder Channel 5 have been doing the same...       0   \n",
       "\n",
       "                                             vectorized  \n",
       "3637  [-0.807587, -0.5216827, -0.8024632, 0.69634426...  \n",
       "1137  [-0.8450406, -0.6705742, -0.9464853, 0.7645291...  \n",
       "5088  [-0.7676656, -0.44996068, -0.8376965, 0.653483...  \n",
       "611   [-0.5953094, -0.39899623, -0.8330214, 0.410473...  \n",
       "6550  [-0.8154489, -0.5718806, -0.94418716, 0.745205...  \n",
       "...                                                 ...  \n",
       "5145  [-0.8021079, -0.5850645, -0.96756816, 0.676609...  \n",
       "5635  [-0.85007364, -0.56564945, -0.9505792, 0.77903...  \n",
       "3023  [-0.39099488, -0.285781, -0.84145904, 0.274905...  \n",
       "855   [-0.9279789, -0.540044, -0.9374038, 0.8930397,...  \n",
       "5715  [-0.7831831, -0.44527027, -0.930498, 0.5956335...  \n",
       "\n",
       "[2000 rows x 6 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [-0.807587   -0.5216827  -0.8024632   0.696344...\n",
       "1    [-0.8450406  -0.6705742  -0.9464853   0.764529...\n",
       "Name: vectorized, dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_small2[\"vectorized\"][0:2]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_small[\"vectorized\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_small[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "^C\n",
      "\u001b[31mOperation cancelled by user\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=1000, n_jobs=-1, random_state=42) #TODO params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = len(X.values[0])\n",
    "num_observations = len(X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.vstack(X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "                       n_jobs=-1, oob_score=False, random_state=42, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X1, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00        24\\n           1       1.00      1.00      1.00        26\\n\\n    accuracy                           1.00        50\\n   macro avg       1.00      1.00      1.00        50\\nweighted avg       1.00      1.00      1.00        50\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "classification_report(y1, clf.predict(X1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"./data/input/test.csv\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3263"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"vectorized\"] = test[\"text\"].map(vectorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(\"./data/work/test-vectorized.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_to_predict = test[\"vectorized\"]\n",
    "X_to_predict = np.vstack(X_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.88337725, -0.43604568, -0.7839005 , ..., -0.62412053,\n",
       "        -0.6474962 ,  0.8741402 ],\n",
       "       [-0.9134144 , -0.7204528 , -0.9842551 , ..., -0.91506916,\n",
       "        -0.79635906,  0.8841588 ],\n",
       "       [-0.8509606 , -0.54145586, -0.97840333, ..., -0.7116491 ,\n",
       "        -0.8331551 ,  0.9537944 ],\n",
       "       ...,\n",
       "       [-0.86718804, -0.5773119 , -0.96989584, ..., -0.7826181 ,\n",
       "        -0.79885864,  0.78483367],\n",
       "       [-0.82174265, -0.52890515, -0.9784757 , ..., -0.8178168 ,\n",
       "        -0.7279344 ,  0.75817096],\n",
       "       [-0.7993479 , -0.5381027 , -0.97783107, ..., -0.82674366,\n",
       "        -0.70411426,  0.7400354 ]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_to_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf.predict(X_to_predict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3263"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3263"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"target\"] = predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "(test[[\"id\", \"target\"]]).to_csv(\"./data/output/result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  target\n",
       "0         0       0\n",
       "1         2       0\n",
       "2         3       0\n",
       "3         9       1\n",
       "4        11       1\n",
       "...     ...     ...\n",
       "3258  10861       0\n",
       "3259  10865       1\n",
       "3260  10868       1\n",
       "3261  10874       1\n",
       "3262  10875       1\n",
       "\n",
       "[3263 rows x 2 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "BERT Embeddings with TensorFlow 2.0",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py36bert",
   "language": "python",
   "name": "py36bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
